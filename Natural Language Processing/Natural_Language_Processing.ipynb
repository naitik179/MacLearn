{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Q8tmRHKq1Q",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw_pPV-cKvaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouDg5jrvLmTI",
        "colab_type": "text"
      },
      "source": [
        "Quoting  == 3 ignores the double quotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELPaRiy1LC3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv' , delimiter='\\t' , quoting= 3)\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ENEYIoMEv9",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning the Text\n",
        "\n",
        "\n",
        "re function used here will eliminate all the unnecessary words and will allow only the small and upper case Alphabets and then adding space between the words .\n",
        "\n",
        "nltk contains the list of words called stopwords which are not uselful in classification of the words as positive or negative so we can eliminate those words directly "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu97lDw-MeEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "corpus = [] # for storing all the cleaned reviews of our dataset\n",
        "for i in range (0 ,1000):\n",
        "\n",
        "  review = re.sub('[^a-zA-Z]' , ' ' , dataset['Review'][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  ps = PorterStemmer() # creating object to apply stemming to each of the word in the review\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "\n",
        "corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU7G-yUpT_U3",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Bag of words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc8jboT-UDox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features= 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[: , 1].values\n",
        "\n",
        "y\n",
        "X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIhRtWiYTfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7c4410c7-2ddb-497c-cf3e-0ca0eb9150c3"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "\n",
        "# Training the Naive Bayes model on the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[55 42]\n",
            " [12 91]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}